# í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰í•´ì„œ URL ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
# crawling\news_searcher.py

import random
import requests
import json
import urllib.parse
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv
import httpx
import asyncio

load_dotenv()  # .env íŒŒì¼ ë¡œë“œ

client_id = os.getenv("NAVER2_CLIENT_ID")
client_secret = os.getenv("NAVER2_CLIENT_SECRET")

# ì¿¼ë¦¬ì—ì„œ ì™„ì „íˆ ì œê±°í•  ë¶ˆí•„ìš”í•œ ë‹¨ì–´
IRRELEVANT_STOPWORDS = {
    # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´
    'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'ì–´ë–»ê²Œ', 'ìš”ì²­', 'í•´ì£¼ì„¸ìš”', 'ì¢€', 'ê·¸ëƒ¥', 'ë§ì´',
    'ì¡°ê¸ˆ', 'ë­', 'ì–´ë–¤ì§€', 'ì•Œë ¤ì¤˜', 'ìˆì–ì•„', 'ì•„ë‹ˆ', 'ì œë°œ', 'ì•„ì£¼', 'ì •ë§',
    'ë„ˆë¬´', 'ê·¸', 'ì €ê¸°', 'ê·¸ê±°', 'ì´ê±°', 'ê±°ê¸°', 'ì €ê±°', 'ì•Œê² ì–´', 'ì•Œì•˜ì–´', 
    'í˜¹ì‹œ', 'ì•½ê°„', 'ëŒ€ì¶©', 'ìì„¸íˆ', 'ë¹¨ë¦¬', 'ê¸‰í•˜ê²Œ', 'ì²œì²œíˆ', 'ê°„ë‹¨íˆ',
    'ì •í™•íˆ', 'ìˆì–´', 'ì—†ì–´', 'ë³´ì—¬ì¤˜', 'ë³´ì—¬', 'ì¤˜', 'ë¶€íƒí•´', 'ë¶€íƒ', 'ì°¾ì•„ì¤˜',
    'ì°¾ì•„ë´', 'í•´ì¤˜', 'í•´ë´', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ëª¨ë ˆ',
    
    # ì§ˆë¬¸ ê´€ë ¨ ë‹¨ì–´
    'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€', 'ë¬´ì—‡ì„', 'ì™œ', 'ì–´ë–»ê²Œ', 'ì–´ë–¤', 'ì–¼ë§ˆë‚˜',

    # ìš”ì²­ í˜•íƒœì˜ ë¬¸ì¥ ì¢…ê²°ì–´
    'í•´', 'í• ë˜', 'í• ê¹Œ', 'í• ë˜ìš”', 'í• ê¹Œìš”', 'ì£¼ì„¸ìš”', 'ì¤„ë˜', 'ì¤„ê¹Œìš”', 'ì•Œë ¤', 'ë³´ì—¬',

    # ê°íƒ„ì‚¬/ë¶€ê°€ì  ë‹¨ì–´
    'ìŒ', 'ì–´', 'ì•„', 'í—', 'í ', 'ì˜¤', 'ì™€', 'ì´ì•¼', 'ì°¸', 'ì •ë§ë¡œ', 'ì§„ì§œë¡œ', 'ì†”ì§íˆ'
}
# ì‹œê°„ ê´€ë ¨ ë‹¨ì–´
TIME_KEYWORDS = {'ì˜¤ëŠ˜', 'ì–´ì œ', 'ì§€ê¸ˆ', 'ë°©ê¸ˆ', 'ìµœê·¼', 'í˜„ì¬'}
TIME_OFFSET = {'ì˜¤ëŠ˜': 0, 'ì§€ê¸ˆ': 0, 'ë°©ê¸ˆ': 0, 'í˜„ì¬': 0, 'ì–´ì œ': 1, 'ìµœê·¼': 0}

with open("data/korea_location_hierarchy.json", encoding="utf-8") as f:
    LOCATION_MAP = json.load(f)

def clean_keyword(raw: str) -> str:
    """
    IRRELEVANT_STOPWORDS ì œê±°, ì‹œê°„ë‹¨ì–´ëŠ” ë‚¨ê²¨ë‘ 
    """
    tokens = [t for t in raw.split() if t not in IRRELEVANT_STOPWORDS]
    return " ".join(tokens) if tokens else raw

# ìƒ˜í”Œ ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸
person_keywords = [
    # ì •ì¹˜ ì¸ë¬¼
    'ìœ¤ì„ì—´', 'ì´ì¬ëª…', 'ê¹€ê¸°í˜„', 'í•œë™í›ˆ', 'ì´ë‚™ì—°', 'í™ì¤€í‘œ', 'ìœ ìŠ¹ë¯¼', 'ì•ˆì² ìˆ˜', 'ì˜¤ì„¸í›ˆ', 'ì›í¬ë£¡',
    'ì¶”ë¯¸ì• ', 'ë°•ì§€ì›', 'ì‹¬ìƒì •', 'ì¡°êµ­', 'í•œë•ìˆ˜', 'ì •ì€ê²½', 'ë°•ê·¼í˜œ', 'ë¬¸ì¬ì¸', 'ì´ëª…ë°•', 'ë…¸ë¬´í˜„',
    'ì „ë‘í™˜', 'ê¹€ëŒ€ì¤‘', 'ê¹€ì˜ì‚¼',

    # í•´ì™¸ ì •ì¹˜ì¸
    'ë°”ì´ë“ ', 'íŠ¸ëŸ¼í”„', 'ì‹œì§„í•‘', 'ê¹€ì •ì€', 'ê¹€ì—¬ì •', 'í‘¸í‹´', 'ê¸°ì‹œë‹¤', 'ë§ˆí¬ë¡±', 'ë©”ë¥´ì¼ˆ',
    'ì ¤ë ŒìŠ¤í‚¤', 'ë‚˜ë Œë“œë¼ ëª¨ë””', 'ë³´ë¦¬ìŠ¤ ì¡´ìŠ¨', 'ë¦¬ì‹œ ìˆ˜ë‚™', 'ì˜¬ë¼í”„ ìˆ„ì¸ ', 'ì¹´ë§ë¼ í•´ë¦¬ìŠ¤',

    # ê²½ì œê³„ ìœ ëª… ì¸ì‚¬
    'ì´ì¬ìš©', 'ì •ì˜ì„ ', 'ìµœíƒœì›', 'êµ¬ê´‘ëª¨', 'ì‹ ë™ë¹ˆ', 'ì—˜ë¡  ë¨¸ìŠ¤í¬', 'ë§ˆí¬ ì €ì»¤ë²„ê·¸', 'ì œí”„ ë² ì´ì¡°ìŠ¤', 
    'íŒ€ ì¿¡', 'ì†ì •ì˜', 'ì›ŒëŸ° ë²„í•', 'ë¹Œ ê²Œì´ì¸ ', 'ì­ ë§ˆ',

    # ì—°ì˜ˆÂ·ìŠ¤í¬ì¸  ìœ ëª… ì¸ì‚¬
    'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ì—°ì•„', 'ë¥˜í˜„ì§„', 'ì´ì •í›„', 'ì„ì˜ì›…', 'BTS', 'ë¸”ë™í•‘í¬', 'ìœ ì¬ì„', 
    'ì•„ì´ìœ ', 'ì „ì§€í˜„', 'ì†¡ì¤‘ê¸°', 'í˜„ë¹ˆ', 'ì´ë³‘í—Œ', 'ì°¨ì€ìš°',

    # ê¸°íƒ€ ì‚¬íšŒì ìœ¼ë¡œ ìœ ëª…í•œ ì¸ë¬¼
    'ì¡°ì½”ë¹„ì¹˜', 'í˜ë”ëŸ¬', 'ë©”ì‹œ', 'í˜¸ë‚ ë‘', 'ë°•í•­ì„œ', 'ì†ì„í¬', 'ì´êµ­ì¢…', 'ë°±ì¢…ì›', 'ê°•í˜•ìš±',
    'ìœ¤ì—¬ì •', 'ë´‰ì¤€í˜¸', 'í™©ì„ ìš°', 'ì¶”ì‹ ìˆ˜'
]

# ìƒ˜í”Œ ì§€ì—­ ë¦¬ìŠ¤íŠ¸
location_keywords = [
    'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ì¸ì²œ', 'ìš¸ì‚°', 'ëŒ€ì „', 'ì„¸ì¢…', 'ìˆ˜ì›', 'ì„±ë‚¨',
    'ê³ ì–‘', 'ìš©ì¸', 'ì°½ì›', 'ì²­ì£¼', 'ì „ì£¼', 'í¬í•­', 'ì—¬ìˆ˜', 'ì²œì•ˆ', 'ì•ˆì‚°', 'ì œì£¼',
    'ê°•ë¦‰', 'ì¶˜ì²œ', 'í‰ì°½', 'ê²½ì£¼', 'êµ°ì‚°', 'ëª©í¬', 'ì§„ì£¼', 'ê¹€í•´', 'ì–‘ì‚°',
    
    # ë¶í•œ ì£¼ìš” ì§€ì—­ (ë‰´ìŠ¤ì— ìì£¼ ë“±ì¥)
    'í‰ì–‘', 'ê°œì„±', 'í•¨í¥', 'ì‹ ì˜ì£¼',

    # í•´ì™¸ ì£¼ìš” ë„ì‹œ (ìì£¼ ë“±ì¥)
    'ë‰´ìš•', 'ì›Œì‹±í„´', 'ë² ì´ì§•', 'ìƒí•˜ì´', 'ë„ì¿„', 'ì˜¤ì‚¬ì¹´', 'ëŸ°ë˜', 'íŒŒë¦¬', 'ë² ë¥¼ë¦°',
    'ëª¨ìŠ¤í¬ë°”', 'í™ì½©', 'ì‹±ê°€í¬ë¥´', 'ë‘ë°”ì´', 'ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤', 'ì‹¤ë¦¬ì½˜ë°¸ë¦¬'
]

# ìƒ˜í”Œ ê²½ì œ í‚¤ì›Œë“œ
economy_keywords = [
    # êµ­ë‚´ ì£¼ìš” ê¸°ì—…Â·ê·¸ë£¹
    'ì‚¼ì„±', 'LG', 'í˜„ëŒ€ìë™ì°¨', 'ê¸°ì•„', 'SKí•˜ì´ë‹‰ìŠ¤', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„', 'ë¡¯ë°', 'í•œí™”', 'í¬ìŠ¤ì½”', 'ì¿ íŒ¡', 'ì…€íŠ¸ë¦¬ì˜¨',
    'CJ', 'KT', 'ì‹ ì„¸ê³„', 'ì•„ëª¨ë ˆí¼ì‹œí”½', 'í•˜ë‚˜ì€í–‰', 'ì‹ í•œì€í–‰', 'KBêµ­ë¯¼ì€í–‰',

    # í•´ì™¸ ê¸°ì—…
    'ì• í”Œ', 'í…ŒìŠ¬ë¼', 'ì—”ë¹„ë””ì•„', 'ì•„ë§ˆì¡´', 'êµ¬ê¸€', 'ì•ŒíŒŒë²³', 'ë„·í”Œë¦­ìŠ¤', 'ë©”íƒ€', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'í˜ì´ìŠ¤ë¶',
    'í™”ì›¨ì´', 'ìƒ¤ì˜¤ë¯¸', 'í…ì„¼íŠ¸', 'ì•Œë¦¬ë°”ë°”',

    # ì£¼ìš” ê²½ì œì§€í‘œÂ·ë¶„ì•¼
    'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ë‚˜ìŠ¤ë‹¥', 'ë‹¤ìš°ì¡´ìŠ¤', 'S&P500', 'ì£¼ì‹ì‹œì¥', 'ì¦ì‹œ', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ë¬¼ê°€', 'ì¸í”Œë ˆì´ì…˜',
    'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸ ê°€ê²©', 'ì „ì„¸', 'ì›”ì„¸', 'ì£¼íƒì‹œì¥', 'ì²­ì•½', 'ë¶„ì–‘',

    # ê¸ˆìœµÂ·ê¸°ìˆ  ì´ìŠˆ
    'ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€', 'ì•”í˜¸í™”í', 'ê°€ìƒí™”í', 'NFT', 'ë¸”ë¡ì²´ì¸', 'í•€í…Œí¬', 'ìŠ¤íƒ€íŠ¸ì—…', 'ì°½ì—…', 'ë²¤ì²˜ê¸°ì—…',
    'ìœ ë‹ˆì½˜', 'IPO', 'ê³µëª¨ì£¼', 'í•œêµ­ì€í–‰', 'ë¯¸êµ­ ì¦ì‹œ', 'ì¤‘êµ­ ê²½ì œ', 'ë¬´ì—­ìˆ˜ì§€', 'ìˆ˜ì¶œì…',

    # ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©
    'ê²½ì œ ìœ„ê¸°', 'ì¹¨ì²´', 'í˜¸í™©', 'ë¶ˆí™©', 'êµ¬ì¡°ì¡°ì •', 'ì‹¤ì—…ë¥ ', 'ê³ ìš©', 'ê²½ê¸°ì¹¨ì²´', 'ë°˜ë„ì²´', 'ìë™ì°¨ ì‚°ì—…', 'ì¡°ì„ ì—…'
]

environment_keywords = {"ë¯¸ì„¸ë¨¼ì§€", "í™©ì‚¬", "ì´ˆë¯¸ì„¸ë¨¼ì§€", "ëŒ€ê¸°ì§ˆ"}


def refine_keyword_for_search(keyword: str) -> str:
    """
    í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ì •ì œí•˜ëŠ” í•¨ìˆ˜
    - í‚¤ì›Œë“œ ì¢…ë¥˜(ì¸ë¬¼, ì‚¬ê±´, ì§€ì—­, ê²½ì œ ë“±)ì— ë”°ë¼ ë‹¤ë¥¸ íŒ¨í„´ ì ìš©
    """
    person_patterns = [
        f"{keyword} ë°œì–¸",
        f"{keyword} ì¸í„°ë·°",
        f"{keyword} ê´€ë ¨ ê¸°ì‚¬",
        f"{keyword} ìµœê·¼ í–‰ë³´"
    ]
    location_patterns = [
        f"{keyword} ì§€ì—­ ë‰´ìŠ¤",
        f"{keyword} í˜„ì§€ ì†Œì‹",
        f"{keyword} ìµœê·¼ ì†Œì‹",
        f"{keyword} ì´ìŠˆ"
    ]
    economy_patterns = [
        f"{keyword} ì£¼ê°€ ë‰´ìŠ¤",
        f"{keyword} ì‹œì¥ ë°˜ì‘",
        f"{keyword} ê²½ì œ ë‰´ìŠ¤",
        f"{keyword} ê´€ë ¨ ë™í–¥"
    ]
    environment_patterns = [
        f"{keyword} ë†ë„",
        f"{keyword} ìƒí™©",
        f"{keyword} ì˜ˆë³´",
        f"{keyword} ìƒíƒœ",
        f"{keyword} í™˜ê²½ ë‰´ìŠ¤"
    ]
    general_patterns = [
        f"{keyword} ìµœì‹  ë‰´ìŠ¤",
        f"{keyword} ê´€ë ¨ ì´ìŠˆ",
        f"{keyword} ìµœì‹  ì†Œì‹",
        f"{keyword} ì´ìŠˆ ì •ë¦¬"
    ]

    if keyword in person_keywords or 'ëŒ€í†µë ¹' in keyword:
        return random.choice(person_patterns)
    elif keyword in location_keywords:
        return random.choice(location_patterns)
    elif keyword in economy_keywords:
        return random.choice(economy_patterns)
    elif keyword in environment_keywords:
        return random.choice(environment_patterns)
    else:
        return random.choice(general_patterns)
    



def build_and_query(tokens: list[str]) -> str:
    """
    ['ë¯¸ì•„ë™', 'ë‚ ì”¨'] â†’ '"ë¯¸ì•„ë™"+"ë‚ ì”¨"' (ì •í™• AND ê²€ìƒ‰)
    """
    return urllib.parse.quote("+".join(f'"{t}"' for t in tokens if t))


def expand_location(token: str) -> list[str]:
    """
    í–‰ì •ë™ ìœ„ì¹˜ í† í°ì„ ['ë™', 'êµ¬', 'ì‹œ', 'ëŒ€í•œë¯¼êµ­'] ë“±ìœ¼ë¡œ í™•ì¥
    - JSON ê¸°ë°˜ ë§¤í•‘: ê°’ì´ ë¦¬ìŠ¤íŠ¸(ë¶€ëª¨ ê³„ì¸µ) í˜•íƒœì´ë¯€ë¡œ,
      í•œë²ˆì— ë¶ˆëŸ¬ì™€ì„œ chainì— ë¶™ì—¬ì¤ë‹ˆë‹¤.
    """
    chain = [token]
    # LOCATION_MAP[token]ì´ ë°”ë¡œ ['êµ¬','ì‹œ','ëŒ€í•œë¯¼êµ­'] ë¦¬ìŠ¤íŠ¸
    parents = LOCATION_MAP.get(token, [])
    for parent in parents:
        if parent and parent not in chain:
            chain.append(parent)
    # ì•ˆì „ì¥ì¹˜: ë§ˆì§€ë§‰ì— 'ëŒ€í•œë¯¼êµ­'ì´ ì—†ìœ¼ë©´ ì¶”ê°€
    if chain[-1] != "ëŒ€í•œë¯¼êµ­":
        chain.append("ëŒ€í•œë¯¼êµ­")
    return chain

async def _fetch_items(client: httpx.AsyncClient, url: str) -> list[dict]:
    """ë‹¨ì¼ URLì—ì„œ JSON ì•„ì´í…œì„ ê°€ì ¸ì˜¤ê³  ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë¦¬í„´"""
    try:
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        r = await client.get(url, headers=headers)
        r.raise_for_status()
        return r.json().get("items", [])
    except Exception:
        return []

async def _fetch_for_keyword(
    client: httpx.AsyncClient,
    raw_keyword: str,
    max_per_keyword: int
) -> tuple[str, list[str]]:
    # 1) ê¸°ì¡´ clean_keyword, token ë¶„ë¦¬, date_offset ê³„ì‚°
    keyword = clean_keyword(raw_keyword if isinstance(raw_keyword, str) else " ".join(raw_keyword))
    tokens = keyword.split()
    date_offset = next((TIME_OFFSET[t] for t in tokens if t in TIME_KEYWORDS), None)

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    urls_to_try: list[str] = []

    # 2) ìœ„ì¹˜ í‚¤ì›Œë“œ ìˆì„ ë•Œ
    loc = next((t for t in tokens if t in LOCATION_MAP), None)
    if loc:
        for loc_variant in expand_location(loc):
            query = build_and_query([loc_variant] + [t for t in tokens if t != loc])
            urls_to_try.append(
                f"https://openapi.naver.com/v1/search/news"
                f"?query={query}&display={max_per_keyword}&sort=date"
            )
    # 3) ìœ„ì¹˜ ì—†ìœ¼ë©´ ì¼ë°˜ í‚¤ì›Œë“œ íŒ¨í„´
    else:
        pattern = refine_keyword_for_search(keyword)
        q = urllib.parse.quote(pattern)
        urls_to_try.append(
            f"https://openapi.naver.com/v1/search/news"
            f"?query={q}&display={max_per_keyword}&sort=date"
        )

    # 4) í›„ë³´ URLë“¤ ë³‘ë ¬ ìš”ì²­ â†’ ì²« ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    tasks = [_fetch_items(client, url) for url in urls_to_try]
    for items in await asyncio.gather(*tasks):
        if items:
            break
    else:
        items = []

    # 5) ë‚ ì§œ í•„í„°ë§
    if date_offset is not None:
        target = (datetime.now() - timedelta(days=date_offset)).date()
        items = [
            it for it in items
            if datetime.strptime(it.get("pubDate", ""), "%a, %d %b %Y %H:%M:%S %z").date() == target
        ]
    print(f"ğŸ§ª refined query: {pattern}")
    print(f"ğŸ”— request URL: {urls_to_try}")

    return keyword, [it["link"] for it in items]

async def search_news_by_keywords(
    keywords: list[str],
    max_per_keyword: int = 3
) -> dict[str, list[str]]:
    """
    ë¹„ë™ê¸° httpx.AsyncClient + asyncio.gatherë¡œ
    í‚¤ì›Œë“œë³„Â·ìœ„ì¹˜ë³„ ê²€ìƒ‰ì„ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    results: dict[str, list[str]] = {}
    async with httpx.AsyncClient(timeout=5.0) as client:
        tasks = [
            _fetch_for_keyword(client, kw, max_per_keyword)
            for kw in keywords
        ]
        for keyword, links in await asyncio.gather(*tasks):
            results[keyword] = links
    return results