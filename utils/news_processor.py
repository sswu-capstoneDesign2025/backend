# í˜¹ì‹œ ë” ë¹ ë¥¸ ì²˜ë¦¬ê°€ ë ê¹Œí•´ì„œ ë§Œë“  gpt apiìš© í•¨ìˆ˜(í›¨ì”¬ ë¹¨ë¼ì§!! ì„±ê³µ!!)
# utils\news_processor.py

import openai
import os
from dotenv import load_dotenv
from openai import AsyncOpenAI
import re
import pickle
import tiktoken
import json

CACHE_PATH = "summary_cache.pkl"
SUMMARY_CACHE = {}
if os.path.exists(CACHE_PATH):
    try:
        with open(CACHE_PATH, "rb") as f:
            SUMMARY_CACHE = pickle.load(f)
        print(f"ğŸ“‚ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(SUMMARY_CACHE)}ê°œ")
    except Exception as e:
        print(f"â— ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
client = AsyncOpenAI()

# -------------------
# NEW: í† í° ìˆ˜ ê³„ì‚°ìš©
# -------------------
def count_tokens(text: str, model: str = "gpt-4o") -> int:
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

# --------------------------
# NEW: ìë™ ì²­í¬ ë¶„í•  ë¡œì§
# --------------------------
def chunk_url_text_pairs(
    url_text_pairs: list[tuple[str, str]],
    model: str = "gpt-4o",
    max_tokens: int = 28000
) -> list[list[str]]:
    """
    [(url, text), ...] ë¥¼ model í•œë„(max_tokens)ì— ë§ì¶°
    í…ìŠ¤íŠ¸ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„í• í•´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    # ê° ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ 'ë¸”ë¡' ë¬¸ìì—´ë¡œ ë³€í™˜
    blocks = [
        f"=== ê¸°ì‚¬ {i} ({url}) ===\n{txt.strip()}\n"
        for i, (url, txt) in enumerate(url_text_pairs, start=1)
    ]
    chunks = []
    current_chunk = []
    current_text = ""
    for block in blocks:
        # í˜„ì¬ ì²­í¬ì— blockì„ ì¶”ê°€í–ˆì„ ë•Œ í•œë„ ì´ˆê³¼ ì—¬ë¶€ ê²€ì‚¬
        if count_tokens(current_text + block, model) > max_tokens:
            # ì´ˆê³¼í•˜ë©´ ì§€ê¸ˆê¹Œì§€ ìŒ“ì€ ì²­í¬ë¥¼ í™•ì •í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
            chunks.append(current_chunk)
            current_chunk = [block]
            current_text = block
        else:
            current_chunk.append(block)
            current_text += block
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

# --------------------------------------------------------
# NEW: ìš”ì•½ + ì‰¬ìš´ ì–¸ì–´ ë³€í™˜ â†’ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ + ìµœì¢… í†µí•©
# --------------------------------------------------------
async def MASSaC(
    url_text_pairs: list[tuple[str, str]]
) -> dict[str, list[str] | str]:
    """
    í•¨ìˆ˜ ëœ»: multi_article_simplified_summary_and_combine
    1) í† í° í•œë„ ê²€ì‚¬ â†’ í•„ìš” ì‹œ ìë™ ì²­í¬ ë¶„í• 
    2) ê° ì²­í¬ë³„ë¡œ 500ì ìš”ì•½ + ì‰¬ìš´ ë§íˆ¬ ì¬ì‘ì„± â†’ summaries ìˆ˜ì§‘
    3) ëª¨ë“  summariesë¥¼ ë§ˆì§€ë§‰ì— í•œ ë²ˆì˜ í”„ë¡¬í”„íŠ¸ë¡œ í†µí•©
    - ë°˜í™˜: { "summaries": [...], "combined": "..." }
    """
    model = "gpt-4o"
    # ëª¨ë¸ í•œë„ ëŒ€ë¹„ ì—¬ìœ ë¶„ ë‘” ì„ê³„ì¹˜ (ì˜ˆ: 32K í•œë„ ì¤‘ 28K í† í°ìœ¼ë¡œ)
    max_input_tokens = 28_000

    print(f"ğŸ“Š ë°›ì€ ê¸°ì‚¬ ìˆ˜: {len(url_text_pairs)}ê°œ")
    # 1) ì²­í¬ ë¶„í• 
    chunks = chunk_url_text_pairs(url_text_pairs, model, max_input_tokens)

    all_summaries: list[str] = []
    
    # 2) ì²­í¬ë³„ ìš”ì•½ + ì‰¬ìš´ ë§íˆ¬ ë³€í™˜ (summariesë§Œ)
    for chunk in chunks:
        joined = "\n".join(chunk)
        prompt = f"""
        ë‹¤ìŒ {len(chunk)}ê±´ì˜ ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”:
        1) 1000ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½
        2) ê²½ê³„ì„  ì§€ëŠ¥í˜• ì¥ì• ì¸ ìˆ˜ì¤€ì˜ ì‚¬ìš©ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ì‰¬ìš´ ë§ë¡œ ë°”ê¿”ì£¼ì„¸ìš”
        3) í•„ìš”í•˜ë‹¤ë©´ ì–´ë ¤ìš´ ë§ ì•ì— ì¶”ê°€ ì„¤ëª…ì„ ë„£ì–´ì£¼ì„¸ìš”.(ex. ë°°í„°ë¦¬ì˜ í•œ ì¢…ë¥˜ì¸ ë‚©ì¶•ì „ì§€)

        [ê¸°ì‚¬ ë¸”ë¡]
        {joined}

        [ì¶œë ¥ í˜•ì‹ - JSON]
        {{
        "summaries": [
            "ê¸°ì‚¬1 ìš”ì•½+ì‰¬ìš´ ë¬¸ì¥",
            "ê¸°ì‚¬2 ìš”ì•½+ì‰¬ìš´ ë¬¸ì¥",
            ...
        ]
        }}
        """
        resp = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì–´ë¦°ì´ìš© ë‰´ìŠ¤ í¸ì§‘ìì•¼."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=1500
        )
        out = resp.choices[0].message.content
        # JSON ë¸”ë¡ íŒŒì‹±
        m = re.search(r"\{\s*\"summaries\"\s*:\s*\[([\s\S]*?)\]\s*\}", out)
        if m:
            obj = json.loads("{" + m.group(0).split("{", 1)[1])
            all_summaries.extend(obj["summaries"])

    if not all_summaries:
        return {
            "summaries": [],
            "combined": "ìš”ì•½í•  ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        }
    
    # 3) ìµœì¢… í†µí•©
    combined_prompt = f"""
    ì•„ë˜ëŠ” ì—¬ëŸ¬ ë‰´ìŠ¤ ìš”ì•½ì…ë‹ˆë‹¤.
    - ê²½ê³„ì„  ì§€ëŠ¥í˜• ì¥ì• ì¸ ìˆ˜ì¤€ì˜ ì‚¬ìš©ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œ ë¬¸ì¥ì— í•˜ë‚˜ì˜ ì •ë³´ë§Œ ë‹´ê³ , ì–´ë ¤ìš´ ë§ì€ ì‰¬ìš´ ë§ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
    - ì¤‘ë³µ ì—†ì´ í•˜ë‚˜ì˜ ì‰½ê³  ëª…í™•í•œ ì´ì•¼ê¸°ë¡œ ì´ì–´ ë¶™ì´ì„¸ìš”.
    - ë§ˆì§€ë§‰ì— 'ì´ìƒì´ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ì…ë‹ˆë‹¤.'ë¡œ ë§ˆë¬´ë¦¬.

    [ìš”ì•½ ëª©ë¡]
    {json.dumps(all_summaries, ensure_ascii=False, indent=2)}

    [ì¶œë ¥ í˜•ì‹ - JSON]
    {{ "combined": "ì—¬ê¸°ì— í†µí•© ê²°ê³¼ë¥¼ ì“°ì„¸ìš”" }}
    """
    resp2 = await client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì–´ë¦°ì´ìš© ë‰´ìŠ¤ í¸ì§‘ìì•¼."},
            {"role": "user", "content": combined_prompt}
        ],
        temperature=0.2,
        max_tokens=800
    )
    out2 = resp2.choices[0].message.content
    m2 = re.search(r"\{[\s\S]+\}$", out2)
    combined = ""
    try:
        m2 = re.search(r"\{[\s\S]+\}", out2)
        if m2:
            parsed = json.loads(m2.group(0))
            combined = parsed.get("combined", "")
        else:
            combined = "ìš”ì•½ì„ í†µí•©í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    except Exception as e:
        print(f"â— í†µí•© ìš”ì•½ íŒŒì‹± ì‹¤íŒ¨: {e}")
        combined = "ìš”ì•½ì„ í†µí•©í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    return {
        "summaries": all_summaries,
        "combined": combined
    }
